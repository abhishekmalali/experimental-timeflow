{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import timesynth as ts\n",
    "import numpy as np\n",
    "import ujson as json\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for building the dictionary\n",
    "def create_data_dictionary(phi, sigma, irregular_time_samples, signal):\n",
    "    data_dict = {}\n",
    "    data_dict['phi'] = phi\n",
    "    data_dict['sigma'] = sigma\n",
    "    data_dict['time_samples'] = list(irregular_time_samples)\n",
    "    data_dict['signal'] = list(signal)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(folder_name, file_name, data_dict):\n",
    "    with open(folder_name + file_name + '.json', 'w') as fp:\n",
    "        json.dump(data_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(phi, sigma, num_points):\n",
    "    time_sampler = ts.TimeSampler(stop_time=20)\n",
    "    irregular_time_samples = time_sampler.sample_irregular_time(num_points=num_points,\n",
    "                                                                keep_percentage=50)\n",
    "    irregular_time_samples_diff = np.diff(np.append(0, irregular_time_samples))\n",
    "    signal = np.zeros(len(irregular_time_samples)+1)\n",
    "    noise_samples = np.random.normal(size=len(irregular_time_samples))\n",
    "    for i in range(len(irregular_time_samples)):\n",
    "        signal[i+1] = np.power(phi, irregular_time_samples_diff[i])*signal[i] + \\\n",
    "                        sigma*np.sqrt(1 - np.power(phi, 2*irregular_time_samples_diff[i]))*noise_samples[i]\n",
    "    return irregular_time_samples, signal[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only run to generate datasets\n",
    "# Change folder_name argument for write_file function to generate the dataset\n",
    "num_samples = 10000\n",
    "phi_samples = np.random.uniform(size=num_samples)\n",
    "sigma_samples = np.random.uniform(size=num_samples)\n",
    "signal_lengths = np.random.randint(low=500, high=1000, size=num_samples)\n",
    "for i in tqdm.tqdm(range(num_samples)):\n",
    "    sigma = sigma_samples[i]\n",
    "    phi = phi_samples[i]\n",
    "    sig_length = signal_lengths[i]\n",
    "    signal = np.nan\n",
    "    while np.any(np.isnan(signal)):\n",
    "        time_samples, signal = generate_data(phi, sigma, 600)\n",
    "    data_dict = create_data_dictionary(phi, sigma, time_samples, signal)\n",
    "    file_name = 'series_'+str(i)\n",
    "    write_file('../data/corr/train/', file_name, data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building model for learning autocorrelation of irregular time series residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ujson as json\n",
    "import tqdm\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('/Users/abhishek/Projects/experimental-timeflow/')\n",
    "import timeflow as tflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(folder_name, file_name):\n",
    "    with open(folder_name + file_name+'.json', 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMLayerBatch(object):\n",
    "\n",
    "    def __init__(self, input_size, hidden_layer_size, input_placeholder):\n",
    "\n",
    "        # Initialization of given values\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.target_size = target_size\n",
    "\n",
    "        # Weights and Bias for input and hidden tensor\n",
    "        self.Wi = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Ui = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bi = tf.Variable(tf.zeros([self.hidden_layer_size]))\n",
    "\n",
    "        \n",
    "        self.Wf = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Uf = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bf = tf.Variable(tf.zeros([self.hidden_layer_size]))        \n",
    "        \n",
    "        \n",
    "        self.Wog = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Uog = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bog = tf.Variable(tf.zeros([self.hidden_layer_size]))        \n",
    "        \n",
    "        \n",
    "        self.Wc = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Uc = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bc = tf.Variable(tf.zeros([self.hidden_layer_size]))        \n",
    "        \n",
    "        # Placeholder for input vector with shape[batch, seq, embeddings]\n",
    "        self._inputs = input_placeholder\n",
    "\n",
    "        # Processing inputs to work with scan function\n",
    "        self.processed_input = process_batch_input_for_RNN(self._inputs)\n",
    "        \n",
    "        self.initial_hidden = self._inputs[:, 0, :]\n",
    "        self.initial_hidden= tf.matmul(\n",
    "            self.initial_hidden, tf.zeros([input_size, hidden_layer_size]))\n",
    "        \n",
    "        \n",
    "        self.initial_hidden=tf.pack([self.initial_hidden,self.initial_hidden])\n",
    "        \n",
    "    # Function for LSTM cell.\n",
    "    def forward_step(self, previous_hidden_memory_tuple, x):\n",
    "        \"\"\"\n",
    "        This function takes previous hidden state and memory tuple with input and\n",
    "        outputs current hidden state.\n",
    "        \"\"\"\n",
    "        \n",
    "        previous_hidden_state,c_prev=tf.unpack(previous_hidden_memory_tuple)\n",
    "        \n",
    "        #Input Gate\n",
    "        i= tf.sigmoid(\n",
    "            tf.matmul(x,self.Wi)+tf.matmul(previous_hidden_state,self.Ui) + self.bi \n",
    "        )\n",
    "        \n",
    "        #Forget Gate\n",
    "        f= tf.sigmoid(\n",
    "            tf.matmul(x,self.Wf)+tf.matmul(previous_hidden_state,self.Uf) + self.bf \n",
    "        )\n",
    "        \n",
    "        #Output Gate\n",
    "        o= tf.sigmoid(\n",
    "            tf.matmul(x,self.Wog)+tf.matmul(previous_hidden_state,self.Uog) + self.bog\n",
    "        )\n",
    "        \n",
    "        #New Memory Cell\n",
    "        c_= tf.nn.tanh(\n",
    "            tf.matmul(x,self.Wc)+tf.matmul(previous_hidden_state,self.Uc) + self.bc \n",
    "        ) \n",
    "        \n",
    "        #Final Memory cell\n",
    "        c= f*c_prev + i*c_\n",
    "        \n",
    "        #Current Hidden state\n",
    "        current_hidden_state = o*tf.nn.tanh(c)\n",
    "\n",
    "\n",
    "        return tf.pack([current_hidden_state,c])\n",
    "\n",
    "    # Function for getting all hidden state.\n",
    "    def get_outputs(self):\n",
    "        \"\"\"\n",
    "        Iterates through time/ sequence to get all hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        # Getting all hidden state throuh time\n",
    "        all_hidden_states = tf.scan(self.forward_step,\n",
    "                                    self.processed_input,\n",
    "                                    initializer=self.initial_hidden,\n",
    "                                    name='states')\n",
    "        all_hidden_states=all_hidden_states[:,0,:,:]\n",
    "        \n",
    "        return all_hidden_states\n",
    "\n",
    "# Function to convert batch input data to use scan ops of tensorflow.\n",
    "def process_batch_input_for_RNN(batch_input):\n",
    "    \"\"\"\n",
    "    Process tensor of size [5,3,2] to [3,5,2]\n",
    "    \"\"\"\n",
    "    batch_input_ = tf.transpose(batch_input, perm=[2, 0, 1])\n",
    "    X = tf.transpose(batch_input_)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanLayerBatch(object):\n",
    "    \n",
    "    def __init__(self, input_layer):\n",
    "        self.inputs = input_layer.get_outputs()\n",
    "    \n",
    "    def get_outputs(self):\n",
    "        return tf.reduce_mean(self.inputs, reduction_indices=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegLayerBatch(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, input_layer):\n",
    "        self.inputs = input_layer.get_outputs()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.Wo = tf.Variable(tf.truncated_normal([self.input_size, self.output_size], mean=0, stddev=.01))\n",
    "        self.bo = tf.Variable(tf.truncated_normal([self.output_size], mean=0, stddev=.01))\n",
    "    \n",
    "    def get_outputs(self):\n",
    "        output = tf.matmul(self.inputs, self.Wo) + self.bo\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 10\n",
    "input_size = 2\n",
    "target_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_placeholder = tf.placeholder(tf.float32,\n",
    "                                   shape=[None, None, input_size],\n",
    "                                   name='inputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('LSTM_layer'):\n",
    "    lstm_layer = LSTMLayerBatch(input_size, hidden_layer_size, input_placeholder)\n",
    "with tf.variable_scope('Mean_Layer'):\n",
    "    mean_layer = MeanLayerBatch(lstm_layer)\n",
    "with tf.variable_scope('Reg_Layer'):\n",
    "    reg_layer = RegLayerBatch(hidden_layer_size, target_size, mean_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, shape=[None, target_size],name='inputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputs = reg_layer.get_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('RMSE'):\n",
    "    rmse = tflow.utils.metrics.RMSE(outputs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.summary.scalar(\"RMSE\", rmse)\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training with Adadelta Optimizer\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.02).minimize(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Starting tensorflow session\n",
    "sess=tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting up log directory for tensorboard\n",
    "logs_path = '../tmp/corr/1'\n",
    "writer = tf.train.SummaryWriter(logs_path, graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_random_batch_input(batch_size):\n",
    "    outputs = np.zeros(batch_size)\n",
    "    counter = 0\n",
    "    random_files = np.random.choice(10000, size=batch_size, replace=False)\n",
    "    for k in range(batch_size):\n",
    "        data_dict = read_file('../data/corr/train/', 'series_'+str(random_files[k]))\n",
    "        time_samples = data_dict['time_samples']\n",
    "        samples = data_dict['signal']\n",
    "        outputs[counter] = data_dict['phi']\n",
    "        if counter == 0:\n",
    "            input_vector = np.transpose(np.concatenate(\n",
    "                                    (np.array(time_samples, ndmin=2),\n",
    "                                     np.array(samples, ndmin=2)), axis=0))\n",
    "        elif counter == 1:\n",
    "            value_vector = np.transpose(np.concatenate(\n",
    "                                    (np.array(time_samples, ndmin=2),\n",
    "                                     np.array(samples, ndmin=2)), axis=0))\n",
    "            input_vector = np.stack((input_vector, value_vector))\n",
    "        else:\n",
    "            value_vector = np.transpose(np.concatenate(\n",
    "                                    (np.array(time_samples, ndmin=2),\n",
    "                                     np.array(samples, ndmin=2)), axis=0))\n",
    "            input_vector = np.append(input_vector, np.expand_dims(value_vector, 0), axis=0)\n",
    "        counter += 1\n",
    "    return input_vector, np.reshape(outputs, (batch_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_vector, out_vector = build_random_batch_input(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:37<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm.tqdm(range(50)):\n",
    "    _, summary = sess.run([train_step, summary_op],\n",
    "                         feed_dict={input_placeholder:input_vector,\n",
    "                                    y:out_vector})\n",
    "    writer.add_summary(summary, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset processing and loading pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_integer('threads', 1, 'Number of threads to preprocess the files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_directory = '../data/corr/train'\n",
    "file_path = '%s/*' % train_directory\n",
    "filenames = tf.gfile.Glob(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spacing = np.linspace(0, len(filenames), FLAGS.threads + 1).astype(np.int)\n",
    "ranges = []\n",
    "for i in xrange(len(spacing) - 1):\n",
    "    ranges.append([spacing[i], spacing[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.gfile.FastGFile(filenames[0], 'r') as f:\n",
    "    image_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str_dict = json.loads(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'phi', u'sigma', u'signal', u'time_samples']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5268461639"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_dict['phi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _convert_to_numpy(str_dict):\n",
    "    signal = np.reshape(np.array(str_dict['signal']), (len(str_dict['signal']), 1))\n",
    "    time_samples = np.reshape(np.array(str_dict['time_samples']), (len(str_dict['time_samples']), 1))\n",
    "    return np.concatenate((signal, time_samples), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _convert_to_list(str_dict):\n",
    "    signal = str_dict['signal']\n",
    "    time_samples = str_dict['time_samples']\n",
    "    return [signal, time_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = _convert_to_numpy(str_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataCoder(object):\n",
    "    def __init__(self):\n",
    "        self._sess = tf.Session()\n",
    "        self._data = tf.placeholder(dtype=tf.uint16)\n",
    "        self._decode_data = ops.convert_to_tensor(self._data, dtype=dtypes.uint16)\n",
    "\n",
    "    def decode_data(self, data):\n",
    "        data_tensor = self._sess.run(self._decode_data, feed_dict={self._data: data})\n",
    "        return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coder = DataCoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_tensor = coder.decode_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_coder(str_dict):\n",
    "    data = _convert_to_numpy(str_dict)\n",
    "    label = str_dict['phi']\n",
    "    #Encoding the data into a tensor\n",
    "    data_buffer = ops.convert_to_tensor\n",
    "    return data_buffer, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str_data = data.tostring()\n",
    "data_check = np.fromstring(str_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_decode_single_example(filename):\n",
    "    filename_queue = tf.train.string_input_producer([filename], num_epochs=None)\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    features = tf.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "      'data': tf.FixedLenFeature([], tf.string),\n",
    "      'label': tf.FixedLenFeature([], tf.string)\n",
    "    \n",
    "    })\n",
    "  \n",
    "    data = features['data']\n",
    "    label = features['label']\n",
    "    print type(data), tf.Print(data)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inputs(train_filename, batch_size):\n",
    "    data, label = read_and_decode_single_example(train_filename)\n",
    "    images_batch, labels_batch = tf.train.shuffle_batch(\n",
    "    [data, label], batch_size=batch_size,\n",
    "    capacity=2000,\n",
    "    min_after_dequeue=1000)\n",
    "    return images_batch, labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_filename = '../data/corr/records/train/train.tfrecords'\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_ = inputs(train_filename, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " 'absolute_import',\n",
       " 'division',\n",
       " 'gzip',\n",
       " 'numpy',\n",
       " 'os',\n",
       " 'print_function',\n",
       " 'read_data_sets',\n",
       " 'tempfile',\n",
       " 'tf',\n",
       " 'urllib',\n",
       " 'xrange']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.core.example.feature_pb2.Feature"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.input_producer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
